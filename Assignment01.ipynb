{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "COzteKVCccsu",
        "KEaSwai7mSb9"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedyusef9/deep_learning_course/blob/Assignment01/Assignment01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 01\n",
        "# Image Classification"
      ],
      "metadata": {
        "id": "xfkIODT0cPxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student 1 ID and name,**\n",
        "\n",
        "**Student 2 ID and name**"
      ],
      "metadata": {
        "id": "ioc2Z913h0tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective\n",
        "In this assignment, the goal is to implement a Convolutional Neural Network (CNN) for image classification on the EMNIST Letters dataset. The dataset consists of hand written letters in the English alphabet.\n",
        "\n",
        "\n",
        "### Instructions\n",
        "\n",
        "The assignment is arranged in sections, each section includes its instructions with coding tasks, and open ended questions. The provided code offers a starting point, in your tasks you should add code to sections that have the comment `#TODO` at the top. And **never delete** given code.\n",
        "\n",
        "### Submission Guidelines\n",
        "\n",
        "*   Assignments are done in pairs, include both ids in the filename when submitting (e.g. *HW01_123456789_123456789.ipynb*) and in the begining of file.\n",
        "*   Submit only one Jupyter notebook containing your code modifications, comments, and analysis.\n",
        "*   Include visualizations, graphs, or plots to support your analysis where needed.\n",
        "*   Provide a conclusion summarizing your findings, challenges faced, and potential future improvements.\n",
        "*   Notice the approximate runtime for some code sections.\n",
        "\n",
        "\n",
        "### Important Notes\n",
        "\n",
        "*  Ensure clarity in code comments and explanations for better understanding.\n",
        "*  Experiment, analyze, and document your observations throughout the assignment.\n",
        "*  Feel free to train on GPU (see example in practice 2-3 notebook).\n",
        "*  It is recommended to explore the datasest and understand it well before implementation.\n",
        "*  Feel free to seek clarification on any aspect of the assignment in the provided forum in moodle."
      ],
      "metadata": {
        "id": "o7ItwkwKcpWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J2jCP6VRYnvE"
      },
      "outputs": [],
      "source": [
        "# arrange any/all imports here\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# viz and plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Data Understanding and Preparation\n",
        "\n",
        "In this section we will load, explore and prepare the dataset for training.\n",
        "\n",
        "You are given a dataset of grayscale images, each 28x28 pixels, representing handwritten letters (A-Z). The dataset is called* EMNIST Letters* you can read more about it [here](https://www.kaggle.com/datasets/crawford/emnist). The dataset will be downloaded to local enviroment using the `tourchvision`. Your tasks in this section are:\n",
        "\n",
        "1. Describe how you would preprocess the data for a CNN explain your choice of transofrmation. Implement the preprocessing of your choice in the `transform` code section below.\n",
        "2. Create train loader for the trian and test sets. Explain your choice of `batch_size`."
      ],
      "metadata": {
        "id": "COzteKVCccsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Define transformations for the dataset\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "vP_OsDkyfElA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the EMNIST Letters dataset\n",
        "# loading the set takes approx 3 mins\n",
        "\n",
        "train_dataset = torchvision.datasets.EMNIST(\n",
        "  root='./data',\n",
        "  split='letters',\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.EMNIST(\n",
        "  root='./data',\n",
        "  split='letters',\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=transform\n",
        ")\n",
        "\n",
        "# Inspect the dataset\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "print(f\"Shape of one image: {train_dataset[0][0].shape} (C x H x W)\")"
      ],
      "metadata": {
        "id": "n35RkBUkfIYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a832d75d-e297-4ecf-b4d4-69bbdf08de7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 562M/562M [00:32<00:00, 17.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n",
            "Number of training samples: 124800\n",
            "Number of testing samples: 20800\n",
            "Shape of one image: torch.Size([1, 28, 28]) (C x H x W)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Create data loaders for training and testing\n",
        "import time\n",
        "\n",
        "num_workers=2\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=num_workers)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "\n",
        "# start_time = time.time()\n",
        "# for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "#     pass  # Simulate data processing\n",
        "# end_time = time.time()\n",
        "# print(f\"Num Workers: {num_workers}, Time Taken: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "ZydFCKXZfMYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fccc65e-32f4-424c-eb89-6cc18f0274d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Workers: 2, Time Taken: 39.79 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code section displays the first 8 images in the batch.\n",
        "\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
        "for i in range(8):\n",
        "  axes[i].imshow(images[i].squeeze(0), cmap='gray')\n",
        "  axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
        "  axes[i].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "86n5GePufObd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "420f2cb8-ab60-49e3-e1d7-dab08704367a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x200 with 8 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABOwAAACuCAYAAACFmpLxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvnUlEQVR4nO3deXSV5bX48R2mBEggkjDJEERwACI4MggBJwYFL1aWOFDsKu21ipWy9OLQKrikFpZ6HQAp19JKmV0sQFDEQhXBK+JFyhAUhUCYJJCBJAxCCHl/f/iT8j57Sw4hJ+c9J9/PWv7xbHbOeXLynHd4PGfvOM/zPAEAAAAAAAAQCDUiPQEAAAAAAAAA/8aGHQAAAAAAABAgbNgBAAAAAAAAAcKGHQAAAAAAABAgbNgBAAAAAAAAAcKGHQAAAAAAABAgbNgBAAAAAAAAAcKGHQAAAAAAABAgbNgBAAAAAAAAAVJtNuyys7MlLi5OXn755Up7zFWrVklcXJysWrWq0h4TsY+1iEhg3SFSWHuIBNYdwo01hkhg3SFIWI/hF+gNu7ffflvi4uJk/fr1kZ5KWHzzzTcyevRo6dGjhyQkJEhcXJxkZ2ebuW3atJG4uDj1329+85uqnXQ1FetrUURk3rx5cs0110hCQoI0btxYRowYIXl5eZGeVrXGukOkxPraGzdunHlOTUhIiPTUqrVYX3ciIitXrpSbbrpJUlNTJTk5WW644QaZOXNmpKdVbcT6Glu4cKEMHTpU2rZtK/Xq1ZPLL79cHn/8cSksLFS58+fPl2HDhkn79u0lLi5O+vTpU+XzrS5Yd//Guou8WF+PIrF1f1Er0hOoztauXStvvPGGdOjQQa688krZuHHjOfO7dOkijz/+uC922WWXhXGGqC6mTp0qjzzyiNxyyy3y3//937Jv3z55/fXXZf369bJu3TpuYhEWrDtE2tSpUyUxMfHMuGbNmhGcDWLdkiVLZPDgwdK9e/czm8bvvPOODB8+XPLy8mT06NGRniKi3H/+53/KxRdfLMOGDZPWrVvLli1bZPLkybJs2TLZsGGD1K1b90zu1KlT5csvv5Trr79e8vPzIzhrRDvWHYIk1u4v2LCLoDvvvFMKCwslKSlJXn755XI37Fq0aCHDhg2rmsmh2igpKZFnnnlGMjIyZMWKFRIXFyciIj169JBBgwbJW2+9Jb/97W8jPEvEGtYdgmDIkCGSmpoa6Wmgmpg8ebI0b95cPvroI4mPjxcRkYceekiuuOIKefvtt9mwwwVbsGCB+sTStddeKw8++KDMnj1bfvWrX52Jz5w5U1q0aCE1atSQTp06VfFMEUtYdwiKWLy/CPRXYkNRUlIizz33nFx77bXSsGFDqV+/vvTq1Us+/vjjn/yZV199VdLS0qRu3brSu3dvyczMVDnbtm2TIUOGSKNGjSQhIUGuu+46WbJkSbnzOX78uGzbti2kj1w2atRIkpKSys07W0lJiRw7duy8fgZVI1rXYmZmphQWFsrQoUPPHNRERAYOHCiJiYkyb968cp8LkcO6Q6RE69o7m+d5UlxcLJ7nhfwziKxoXnfFxcVy0UUXndmsExGpVauWpKam+j6BgsiK5jVmfb3wrrvuEhGRr7/+2hdv1aqV1KgR9beCMYN1hyCJ1vUYi/cXUf9uKS4ulr/85S/Sp08fmThxoowbN05yc3OlX79+5ifW/v73v8sbb7whI0eOlKeffloyMzPl5ptvloMHD57J2bp1q3Tr1k2+/vpreeqpp+SVV16R+vXry+DBg2XRokXnnM8XX3whV155pUyePLmyf1X56KOPpF69epKYmCht2rSR119/vdKfAxUXrWvx5MmTIiLmzULdunXlX//6l5SVlYXwCiASWHeIlGhde2dr27atNGzYUJKSkmTYsGG+uSCYonnd9enTR7Zu3SrPPvus7NixQ7KysuSFF16Q9evXy5gxY877tUB4RPMas+Tk5IiI8GnigGPdIUiidT3G5P2FF2B/+9vfPBHx/u///u8nc0pLS72TJ0/6YocPH/aaNm3q/fKXvzwT27VrlyciXt26db19+/adia9bt84TEW/06NFnYrfccouXnp7unThx4kysrKzM69Gjh9e+ffszsY8//tgTEe/jjz9WsbFjx57X7/rSSy95IuLt2rXL/PdBgwZ5EydO9BYvXuxNnz7d69Wrlyci3pgxY87reVAxsbwWc3Nzvbi4OG/EiBG++LZt2zwR8UTEy8vLO+djIDxYd6y7SInlted5nvfaa695jz76qDd79mxvwYIF3qhRo7xatWp57du394qKisr9eYRHrK+7o0ePevfcc48XFxd35jhXr149b/HixeX+LCpHrK8xy4gRI7yaNWt633777U/mdOzY0evdu3eFHh/lY93ZWHeREcvrMRbvL6L+E3Y1a9aUOnXqiIhIWVmZFBQUSGlpqVx33XWyYcMGlT948GBp0aLFmfENN9wgXbt2lWXLlomISEFBgXz00Udyzz33yJEjRyQvL0/y8vIkPz9f+vXrJ9u3b5f9+/f/5Hz69OkjnufJuHHjKvX3XLJkiYwZM0b+4z/+Q375y1/KJ598Iv369TtTSBGRF61rMTU1Ve655x6ZMWOGvPLKK7Jz505Zs2aNDB06VGrXri0iIt9///35vhyoIqw7REq0rj0RkVGjRsmkSZPk/vvvl7vvvltee+01mTFjhmzfvl3efPPN83wlUJWied3Fx8fLZZddJkOGDJG5c+fKrFmz5LrrrpNhw4bJ559/fp6vBMIlmteYa86cOTJ9+nR5/PHHpX379uf986g6rDsESbSux1i8v4j6DTsRkRkzZshVV10lCQkJkpKSIo0bN5b3339fioqKVK510LjsssskOztbRER27NghnufJs88+K40bN/b9N3bsWBEROXToUFh/n1DExcXJ6NGjpbS0VFatWhXp6eD/i9a1OG3aNLn99tvliSeekEsvvVQyMjIkPT1dBg0aJCLi66KI4GHdIVKide1Z7r//fmnWrJmsXLkybM+ByhGt6+7RRx+VpUuXyrx58+Tee++VBx54QFauXCnNmzeXUaNGVcpzoHJE6xo725o1a2TEiBHSr18/+eMf/1jpj4/Kx7pDkETreoy1+4uo7xI7a9Ys+cUvfiGDBw+W//qv/5ImTZpIzZo15U9/+pNkZWWd9+P9+J3mJ554Qvr162fmtGvX7oLmXFlatWolIj/sWCPyonktNmzYUN59913Zs2ePZGdnS1pamqSlpUmPHj2kcePGkpycXCnPg8rHukOkRPPa+ymtWrXinBpw0bruSkpKZPr06TJmzBhfwfXatWvLgAEDZPLkyVJSUnLmEw2InGhdY2fbtGmT3HnnndKpUydZsGCB1KoV9bd8MY91hyCJ5vUYa/cXUf8uWrBggbRt21YWLlzo6wTy406ta/v27Sr27bffSps2bUTkhwLUIj9cQN16662VP+FKtHPnThERady4cYRnApHYWIutW7eW1q1bi4hIYWGhfPnll3L33XdXyXOjYlh3iJRYWHtn8zxPsrOz5eqrr67y50boonXd5efnS2lpqZw+fVr926lTp6SsrMz8N1S9aF1jP8rKypL+/ftLkyZNZNmyZVH3aZLqinWHIIn29SgSO/cXUf+V2Jo1a4rIDxfaP1q3bp2sXbvWzF+8eLHv+9FffPGFrFu3TgYMGCAiIk2aNJE+ffrItGnT5MCBA+rnc3Nzzzmf82mBHaqCggJ1EXfq1CmZMGGC1KlTR2666aZKey5UXKytxaefflpKS0tl9OjRFfp5VA3WHSIlmtee9VhTp06V3Nxc6d+/f7k/j8iJ1nXXpEkTSU5OlkWLFklJScmZ+NGjR2Xp0qVyxRVXmF3tUPWidY2J/NCZs2/fvlKjRg358MMP+Z/6UYR1hyCJ5vVoieb7i6j4hN1f//pXWb58uYqPGjVKBg4cKAsXLpS77rpL7rjjDtm1a5f8+c9/lg4dOsjRo0fVz7Rr10569uwpDz/8sJw8eVJee+01SUlJkTFjxpzJmTJlivTs2VPS09Pl17/+tbRt21YOHjwoa9eulX379smmTZt+cq5ffPGF3HTTTTJ27NhyiyIWFRXJpEmTRETkf//3f0VEZPLkyZKcnCzJycny6KOPisgPDSfGjx8vQ4YMkUsuuUQKCgpkzpw5kpmZKS+++KI0a9as3NcQlSNW1+KECRMkMzNTunbtKrVq1ZLFixfLP/7xDxk/frxcf/31ob9ACAvWHSIlVtdeWlqaDB06VNLT0yUhIUE+/fRTmTdvnnTp0kUeeuih0F8ghEUsrruaNWvKE088IX/4wx+kW7duMnz4cDl9+rRMnz5d9u3bJ7NmzTq/FwkXJBbXmIhI//79ZefOnTJmzBj59NNP5dNPPz3zb02bNpXbbrvtzHj16tWyevVqEfnhZvnYsWMyfvx4ERHJyMiQjIyMcz4Xzh/rjnUXJLG6HmPu/qLK+tFWwI8th3/qv71793plZWXeiy++6KWlpXnx8fHe1Vdf7b333nvegw8+6KWlpZ15rB9bDr/00kveK6+84rVq1cqLj4/3evXq5W3atEk9d1ZWljd8+HCvWbNmXu3atb0WLVp4AwcO9BYsWHAm50JbYP84J+u/s+e+fv16b9CgQV6LFi28OnXqeImJiV7Pnj29d955pyIvKyog1tfie++9591www1eUlKSV69ePa9bt26srwBg3SFSYn3t/epXv/I6dOjgJSUlebVr1/batWvnPfnkk15xcfGFvGy4QLG+7jzP82bPnu3dcMMNXnJysle3bl2va9euvudAeMX6GjvX79a7d29f7tixY38yN9T1jNCw7v6NdRd5sb4eY+3+Is7zzvqcIwAAAAAAAICIivoadgAAAAAAAEAsYcMOAAAAAAAACBA27AAAAAAAAIAAYcMOAAAAAAAACBA27AAAAAAAAIAAYcMOAAAAAAAACBA27AAAAAAAAIAAqRVqYlxcXDjngSjjeV6VPA/rDmerqnUnwtqDH8c8RALrDpHAukMkcI2HSOGYh0gIdd3xCTsAAAAAAAAgQNiwAwAAAAAAAAKEDTsAAAAAAAAgQNiwAwAAAAAAAAIk5KYTAAAAABDLatas6RtbhcHLysoq9FinT5+u+MQAANUOn7ADAAAAAAAAAoQNOwAAAAAAACBA2LADAAAAAAAAAoQadgAAICa49aJERJo1a6ZiBQUFvvGpU6dUzu9//3sVmzp1qm+cm5urcqx6VwCCKT4+vtychx9+WMV2796tYl999ZWKJSYm+sbXXHONytmwYYOKFRUVlTuv0tJSFcvJyVGxEydOlPtYAIBg4hN2AAAAAAAAQICwYQcAAAAAAAAECBt2AAAAAAAAQICwYQcAAAAAAAAESJwXYnXkuLi4cM8FUaSqimqz7nC2qizmztrD2TjmhY/1O6ekpJQbS09PVzlDhw5Vsdtuu03F3KYT7lhEpEOHDio2a9Ys3/iZZ55ROXl5eSpWUaw7REJ1X3dt2rTxjTdu3KhyTp48qWJWo4hatfz9/Ro1aqRyrOOP1QjHdeTIERVbunSpirnNcvLz81XO6dOny32+cOMaD5FS3Y95iIxQ1x2fsAMAAAAAAAAChA07AAAAAAAAIEDYsAMAAAAAAAAChA07AAAAAAAAIEBoOnEONWro/UwrFk5lZWUhxaoaxTkRCRQkRqRwzKs8bhH2jh07qpzf/e53KtajRw/fODk5WeVYMeu8XbNmzXNPUkQOHTpU7mONGTNG5cyfP1/Fvv/++3Kfz8K6QyRU93WXmprqGy9btqzcHBGRpk2bqljt2rV941DvLUJ5bay/04kTJ1Ts/fff943d5jkiIh9++GFIjxVOXOOFl3Wu7du3r4o1aNCgKqZzRnFxsYq5a3b37t0qx2r8UlHV/ZiHyKDpBAAAAAAAABCF2LADAAAAAAAAAoQNOwAAAAAAACBA2LADAAAAAAAAAqRaNJ2Ij49XsYYNG6pYs2bNfONLL71U5bRp00bFEhMTfeNQG1NYzSOOHj3qG+/YsUPlbNmyRcVycnJ8Y6sQZ2UW1KQ4JyKBgsSIFI55FdO5c2cVe/XVV33jbt26qRzrddizZ49vXFRUpHKmTJmiYpmZmSqWkZHhG/fs2VPlfPDBByr25ptv+sbZ2dkq5/HHH1expUuXqlgoWHeIhOq+7tx5paWlqZyUlBQVc48rIroRjlX4v1OnTuXOwXqtrCY71rzcv+euXbtUzrhx41RswYIFvnFlFvm3cI0nctFFF/nG1v2qdd5JSEhQMXetTZ48WeVcf/31KlbVDRYtp06d8o3feecdlfPCCy+oWEFBgYrl5eWV+3zV/ZgX69q1a6dipaWlKrZ//37f2F2HlY2mEwAAAAAAAEAUYsMOAAAAAAAACBA27AAAAAAAAIAAibkadjVr1lQxq/ZEenq6irm1dqyaEpdccomKVWYNuyNHjvjGVu2dFStWqNgXX3zhG7vfwRYROXHiREjzCgXf9UckUN8EkcIxr2JSU1NV7K233vKN+/fvH9JjufWUxo8fr3KsGksHDhwoN69WrVoqp1GjRuXOyTpHWzWHrLla9XdcrDtEAuuuYqzjiPs7WnXnrGOGy7q/6d69u4qNGjVKxdz7G+vvu3nzZhX7xS9+UW5ONNbHFgnG2rPuF++8807feMKECSrHqjf41FNPqViHDh18Y2t9Wtz7xb1796ocq566FXMlJSWp2MCBA1XMvd+uXbu2yrHqi+3evVvFevTo4RtbNe045gWP9VrVqVNHxZo3b65i7lq/7777VE5hYaGKzZ071ze2aiJa+zcVRQ07AAAAAAAAIAqxYQcAAAAAAAAECBt2AAAAAAAAQICwYQcAAAAAAAAESGjVJwPCKrjqFqFs0KCByunVq5eK9enTR8W6dOniG1tFDK1imW5hw1ALSlqFBktKSnxjqzitVXjTtXbtWhXLzs4OaV5AqNyCuVYBXWudnz59OmxzAhB+VkMGq8FERkaGit1xxx2+sXVOs4qbv/zyy77xN998U+48Q1VaWqpihw4dUjH3fL9nzx6Vc9VVV6lYKEXlUf1YReBbtmxZbp61Xq1i61VZxB/238WVm5sbUiwU1jHwnXfeUbFp06b5xoMHD1Y51nHr7bff9o0ffPBBlWMdqxEa6762qKjIN27durXKeeONN1TMOv+6rCYNhw8fVrGpU6f6xvPmzVM5VjF+67Fc1vn+L3/5i4otW7bMN7aaPlqvX5MmTVTMbQ5pNZ1A8Fj3lNbf3MobNGiQb3z//ferHOv86B4bH3vsMZWTlZWlYpXZ2NPCJ+wAAAAAAACAAGHDDgAAAAAAAAgQNuwAAAAAAACAAGHDDgAAAAAAAAiQwDSdcBs11KlTR+VYRZubNWvmG7dv317lWMVV09PTVaxp06a+cUJCgsqxih2GwipsaDWncAsLt2jRQuV07dpVxdxCokePHlU5VnHssrIyPVlUe/Hx8Spmvf9uvPFG37hTp04qxypCO2fOHBWzCtgCCIZ69er5xtOnT1c5AwYMUDGrwUxxcbFvbDWwWLhwoYp99dVX5c4z3Nyi8qtWrVI5VvF2q9FVKI0EEB2sa8OUlBQVc98jI0eOVDkdO3ZUMbdQ+/fff69yOnfurGI0G6t+jh8/rmLPPvusb2yti4ceekjFLr/8ct/4rrvuUjnbt29XMWt9xrK6deuqmHv/5l4vi4g8/fTTKuaeD617UatZjdW4xG3c8Kc//UnlWNfoFW2CEgrrPGcV8Xfva6376JkzZ6rYxIkTVcxqyIPgcc+j1jnUagjqNg0VEbnvvvt840svvVTlWNen9evXL/f59u/fr2InT55Uscps+sQn7AAAAAAAAIAAYcMOAAAAAAAACBA27AAAAAAAAIAAiUgNO6vWh1szy61NJ2LXnXNrdlg53bt3VzGrHpdbN8+qMWfVfHNj1vfzre82WzUP3DolVu0C6/vUbu2wLVu2qJwaNfT+LDXsqh93jYnoWhu9e/dWOVYNJrcez0UXXaRyduzYoWLLly9XMWrYAcFg1aIcMmSIb3zrrbeqHLfmjIjI4sWLVaywsNA3/vWvf61yNm/erGJWvZFIO3LkiIqFWrMW0cm6hh04cKCKDRs2TMV69erlGzdu3FjlhLJWWE84H27NuilTpqicK664QsXcGuA/+9nPVM6iRYtUzDp+RyO3npWIXRf9+eefV7FWrVr5xtZxw7oHc2vKzZgxQ+VMmDBBxaxr6Pz8/HKfLwiseu0tW7Ys9+euvPJKFTtx4oSKVWYtMZw/63wVSq8C93wpYu/zWDXsrLXhsq5Z3fdMTk6OyonEGuMTdgAAAAAAAECAsGEHAAAAAAAABAgbdgAAAAAAAECAsGEHAAAAAAAABEjYm05YBe6thg9uwUmrgKBV9N7NsxoypKamqpjVgMEtGGg1irAKTBcXF59zLCJSVFSkYpdccomKNW3a1De2ijJasaSkJN84MTFR5SC21aql385WE4gOHTqo2Lhx43zjjh07qhx3jYnoZjEW670GoOKsAr6NGjUKKS8vL883to4bc+bMUTG3gK9V9Noqjr13714Ve+qpp3zjaC4IHcq1hIjIrl27VCyoBcCrM6vR18UXX+wbX3PNNSrHej+kpaWpmFV43uW+R0VE9uzZ4xsvWbJE5Rw4cKDcxwZERHJzc1Vs2bJlKuY2F7LWdNeuXVVs69atKhbEpkEu9/ezGqRZr4F1X+b+vkuXLlU5L7zwgoq550y3CL5I7J07UlJSVMzdPzh27JjKeeihh1Rs9+7dlTcxVEi9evV8Y/ccKmIfN9zjjdvYUESkQYMGKmbtNbmysrJUzNqv2bhxo2986NAhlWM1qwg37qQBAAAAAACAAGHDDgAAAAAAAAgQNuwAAAAAAACAAGHDDgAAAAAAAAiQsDedsArVp6enq1i3bt184+uvv17lXHXVVSrmNmmwCgaHWhTabTJhFfDNzMxUsW+//dY3tgoUlpaWqtjNN9+sYm6hxlALibu/I4X+Y5+71q2inp07d1ax3r17q5hbUN5ad6GsKet9ZTVqsd4P0cgq2N+yZcuQ8kKxf/9+FXOPU7FWfBjls4pez58/X8WsY0LPnj19Y6spxNy5c1XMLQY8ffp0lZOdna1ilmg+P7nn39atW6scmk5EnrXG3IYPVlOmhx9+WMUGDRrkG7dq1UrlNG7cOKR5uee+goIClfPkk0+q2Jo1a3zj7777TuVYjdIAi3UfYRX+dxuL5eTkqJwNGzaoWDQ0mLC4v98zzzyjctq2batiO3fuVLEtW7b4xta5lvfsD+69914Vc6+bX3/9dZWzefNmFYvmJlZBZx03rIYr7du3942vvvpqlXPLLbeomLsXZDUNteZgNYFwz5HWda3VdGLHjh2+sdU0NBLHt+i9agYAAAAAAABiEBt2AAAAAAAAQICwYQcAAAAAAAAECBt2AAAAAAAAQICEvelEgwYNVKxTp04qdtttt/nGV1xxhcqxCgS7xQ6tYoShNJgQ0QXeP//8c5Xzz3/+U8XcRhSHDx9WOW5zDBG7cHHHjh194+TkZJWD2OIWx7YaFFjrYMSIEb7x3XffrXKsNWa9jyraFMEtoG0VnX/11VdVzGroEg3cv9UjjzyicoYPH65i1nHQZRWgX7p0qYq5x6VNmzapHKupx8GDB1XMLdQazc1AQl3D1uscbcX/Qz1GWMWAjx8/7htbv7v1nv373//uG1tF7y3WOdlq1OD66quvVCwIfyf3emLPnj0qx5rn0aNHwzanSKro+856jUJ5rNq1a6uYe90kIjJy5EgV6969u29snQtDaR5hrekTJ06o2Pr161XsD3/4g29srfPc3Nxy5wBUtvr166uY+560irtbRdmjlXtv+O6776ocq6FNLFxXVBWrOaS7ByCiC/t/9tlnKocGE+HjNmkS0U1oRESaNWumYoMHD/aNraYTVgPS5s2b+8bWudbav7HuKRcvXuwbW00nSkpKVMy9VrNyIoFP2AEAAAAAAAABwoYdAAAAAAAAECBs2AEAAAAAAAABEvYadqHW2nG/A92oUSOVY9XjCUUo9epEdG2o5cuXq5x169apmPvdaev7zlbdlSNHjqiYVR8C0cn6/n9KSoqK3Xjjjb5x586dVY5Va2fo0KG+cZMmTc53ij/p2LFjKmbVQFu0aJFvvHr1apWzYsUKFbPek9HIqsdp/Y2tGpbWMcH1u9/9TsXcuihWPRXrOGL9/fLz833jTz75ROXs27dPxdy/s1WTzFrHlcmtbXHHHXeoHKt24KpVq1TsN7/5jW8c9NozDRs2VDHrXGud56waqy6rHpf1WKGwasy4dd+s19utXxNU1tyt919xcXFVTCesrPo1Vh3PxMREFfv00099423btqmce++9V8Xc93BSUpLKcevliIi0aNFCxaz5V4S1pj/88EMVGzt2rIq5Neu45gOCi9p0F869Rh0wYIDKseqQbt261Tf+4IMPKndi1Zh1veheU1v3nW6NORGRLl26qJh7Tr744otVjnV97s7LrbksIpKTk6NiVs8Bt2bdt99+q3KiCZ+wAwAAAAAAAAKEDTsAAAAAAAAgQNiwAwAAAAAAAAKEDTsAAAAAAAAgQMLedCJUbrFDdxzqz1nFQK1GEe+++66KrVy50jd2C2OL2MW43ee05m4VYbZioRSitwoeu3OgKGp4uQ0lrEYDvXr1UjGrqLabZxX6tNaUVTTUZa0Vt9GAiF7rS5YsUTmbNm1SMfc98/3336ucWFqL7u/yzDPPqJxp06apWEZGhopZjQNC4RZdt5otWMXV3cY+IiJt2rTxja3isVYTHbeBRf369VWOtY7DyXo/WI0L9u7dq2Khnm+CwlpPVqMmt8C9iH1MqGrRfExwC2inp6erHKtBx/vvvx+2OVWVyy+/XMWef/55FbMahH322We+cVFRkcrp27evioVyTRTKuVBEr33rXHj06FEVcxtYWA2lZs+erWKZmZkqVtFmKu4xynq/W82irGtWwHpfWUXg3WP1li1bVI71XgZ+lJqa6hs/8MADKse6Bhs/frxvzLGsYqzGdBb3fuaqq65SOe3atVMx697CvSe27iO+++47FXPPmVYjQ6t5hHVc2r17t4pFMz5hBwAAAAAAAAQIG3YAAAAAAABAgLBhBwAAAAAAAAQIG3YAAAAAAABAgIS96URpaamKWUV9jxw54htbBQpDLSwcCuux3CLwF198scqxfh+XVZD46quvVrH27durmNuIwirEeerUKRVziw1brzG0hIQE39gtjipiF2m/7rrrfONhw4apHKvphNWcwl0v1t88lELx1trMzs5WsRdffFHF3MKeVjFQa91VtIB2rMjLywsp9uWXX4ZtDk8++aSKWQWlrePNz3/+c9/4pptuUjlWIWqXtTZ27dpV7s9dCLcY9tKlS1WOW+heRGT58uUqFm3ruKCgQMUOHz6sYkH9Xd3GKYcOHVI5ViwI3IYuVmHmzZs3q5j19wk69zrplltuUTlW8yyL1SjFFWpxbJd1/CksLFQxtwj1lClTVI517nMbCbVu3VrlWOfoUM7b1u9sNQ267LLLfOPHHntM5ezYsUPF3nrrLRWzzlGoXpo2bapivXv3VjH3etQq7m6911A91a1bV8VeeeUV39hqlGbdq6xataqyphWzrPvFtLQ039hqjGU1j7jvvvt8Y6uxUajNMt0GhDk5OSrn888/V7F//vOfvrHVdKK4uFjF3D0kkWA0WKtMfMIOAAAAAAAACBA27AAAAAAAAIAAYcMOAAAAAAAACJCw17CzaqlZdTa2bt3qG1vfnW7RooWKubU+rO9zX3LJJSpm1Wpwa9ZZ34l26yZZrJokVg27Tp06qZhb28divabu9/+tegChzD1WhPK9fhGR4cOH+8Y333yzyjlx4oSKufVkWrVqpXKsWoahCLUWjvs3Xrhwocqxvv+/YsUKFbN+R0QHq3ahFbPqaj333HO+sVXXJpTaodYxds+ePSoWzvpp+/fvV7GTJ0+qWCwcB63X26prt2bNmqqYzjlZx8FrrrnGN05OTlY51rnQql1S1dz3SJMmTVROrNR5cs871nHFrVUjYte0cR/Lej2sNexeG65du1blWPUrrZqBbsz6fazaPu5xxLrGc+uBitjHQLf+3T333KNy3Bq5IiLNmjXzjd36uyK6lrGI/feZNGmSbxyEupYIL/ea2KrXbN13FRUV+cbWdQTrp3qy7rN+9rOflRurU6eOypk5c6aK5efnX8DsYo/1elv1Tm+88Ubf2Ko9a+0/tG3b1je2znOh1tV393k2btyoclauXKli69at842tc6h1DR8L1/Xl4RN2AAAAAAAAQICwYQcAAAAAAAAECBt2AAAAAAAAQICwYQcAAAAAAAAESNibTliNG9wiwiK62GnDhg1VTr169cr9OatAulVI3Xqsjh07+sZWIcWKuuiii1TMKqrtFuO0irlar+muXbt84927d6uc6lCU8UdWIc7u3bur2F133eUbu0U3Rey/gbt+rMLqVnFOq3mE+3exnq+kpETFPvvsM994/vz5KsdqPmIV4kf15BYkt9ZLKKxGQqg87rHEbZAkYhfw3b59e9jmFCrreLZhwwbfuGXLlirHakAQBO41xsGDB1XOe++9p2KVeT1RVdy/3dy5c1WOdc3VoEEDFXPPc5mZmSrHatbhFr23mklYzSMqyn0+EX3N2r59e5XTt29fFUtPT1ex+vXr+8ZWof9QGlZZ13MHDhxQsaysLBWzrkMQ2xo3buwbjxw5UuU0b95cxdx7Cev+rTrdW0Q7q5GAdb/kNnm0jkn33Xefio0ZM0bF3J91711ERF566SUV4zjlZzXrcJsRiYj079/fN+7WrVtIP+c2i7Ku3awGhTk5OSq2ePFi39hqOrFp0yYVc6+nKvPcHu34hB0AAAAAAAAQIGzYAQAAAAAAAAHChh0AAAAAAAAQIGzYAQAAAAAAAAES9qYTVrF8q0ChW5DQKqqdnJysYm6xaqtZhVXU12oCYT1+ZbEKfVoxl1WourCwUMXc19TKiVXW69ivXz8Ve/7551Xs0ksv9Y2tRhEVZRVMPXbsmIp99NFHvrFbkF1EpLi4WMUWLVrkG1uNRijaCkQ/931svdetY4TVXMltNBJu1vG5TZs2vrFVWDioTRrc6wTrGsdqXBAL8vLyVGzSpEkVeiyrUH0QitdbTS1Wr17tG/fo0UPlWEW83XUeKquw93fffecbW0W8Z8yYoWIrVqxQsSC8zqhavXr18o0HDBigctyi8yK6EYz1/kB4WQ0fWrVqpWJW84jbb7/dN77xxhtVTpcuXcp9fOveyJpXfn6+ir366qu+8fTp01XO8ePHVaw6s66brEaVVmMjt8mE20BExF4r33zzjW9sNYGymkVZ5yL3vGM1O7SaWuCn8Qk7AAAAAAAAIEDYsAMAAAAAAAAChA07AAAAAAAAIEDYsAMAAAAAAAACJOxNJ6yi91bxwf379/vGn3/+ucqxCva7TSfatm2rcnr27KliVnOKOnXq+MaV2YAgVG4RxiNHjqicnTt3lhuzfi5WWYU4rcKqVmHzHTt2hGVOInYR+KVLl6rYrFmzfOO9e/eqHOt9ZP0+AGKf1SDpgQceULE1a9aomFUUOpys4skdOnTwja1zexBY1wBvvvmmb7x8+XKV417PxLJYOw9Zv8+cOXN8Y6upyMSJE1XMam7mPv6BAwdUzsyZM1XMvXawrhOs93asNpiwiqanpqaqmNsoxbr/iDXWazNs2DDfOCUlReVkZ2ermHt9WlBQcGGTq0Zq1fLfYrv3qyL23yEjI8M3tu5h+/fvr2JW0xC3MYTV0ObgwYMq5q4F6zjy/vvvhxT7+OOPVQznZl17JCYmqpjbOFFEn3es/QCr+efcuXN9482bN6ucrKyskB7LbW5GA8QLxyfsAAAAAAAAgABhww4AAAAAAAAIEDbsAAAAAAAAgAAJew07i1unTUTXBLG+O71r1y4Va9SokW9s1bBzv8MvIpKenq5iTZs29Y3dmnYilVvXLpT6ft99953KyczMVDH3e+XVqYadVf9o8uTJKvY///M/VTGdM6xaONZ3/a2aEgDwI/ccdu2116qc48ePq1hhYWG4phQyq/aNW9Pm66+/VjlBqI2WlpamYu41wLp161ROdaiTVZ24ddBmzJihcpYtW6ZiVq1kd11b9aPc+j/Q+vbtq2K//e1vVWzKlCm+8QcffKBySkpKKm9iVcy6v7nyyitV7NZbb/WNrfuPRYsWqdjKlSt9Y+v+LZa1adNGxdzadCIiXbp0UbGnnnrKN7b+LqHUnbPW5549e1Rsy5YtKrZp0ybf+N1331U527dvV7FTp06pmCsI5+hYZb0/jx49qmJWTbmNGzf6xocPH1Y5u3fvVjG3hp31c9begrVWqFlX+fiEHQAAAAAAABAgbNgBAAAAAAAAAcKGHQAAAAAAABAgbNgBAAAAAAAAARKRphMWt2ihVezQbUwhogv27tu3T+XUr19fxaxi3B07dvSNk5KSVE6NGpW3x2kV4y4uLvaN3YKhIrqgpIhuZhBKwdBYYRU+tQqyAkA0cgt9r1mzRuVYRdiDwDrPPffcc75xNBUo/vnPf+4bT5o0SeW453ERkRUrVqhYNP3eOLfc3NyQYjh/1nW3VfjfaiY3ffp039hqXvfaa6+p2OLFi31jq5GMdWyzYi5r7ha3wY3VBOf3v/+9it1+++3lPvb8+fNVzG3QIWI3dYtlbsOHadOmqRyrEUVycrKKpaam+sahNBoU0fexn3zyicqZMGGCihUUFKiYey9No4joYB1H3OZHIiL/+Mc/VMxtPuI2nBERyc/PV7G9e/f6xtZasZrOhHLMw4XjE3YAAAAAAABAgLBhBwAAAAAAAAQIG3YAAAAAAABAgLBhBwAAAAAAAARInBdi1WO3+GlQuQVDRexCrW6DCRGRdu3a+caJiYkqJ9xNJ44ePeob79ixQ+W4BSVFRA4cOOAbW4VMK1NVFcuOlnWHqlGVRdpZezhbkI551nlo3rx5IT3WI4884htTGP/8ZGVl+cbWuujcubOKVbR4e5DWHaqPIK0763jnFvQXEZk4caKKDR061DeOj49XOdnZ2SrmNp2wmt5Z1+JfffWVirn3EhkZGSqnQYMGKub+3l26dFE5t912m4pZ3NdmxowZKmf37t0qVtWNcSJ9jec2BNm8ebPKce8VRexGiW5zqNmzZ6ucf/3rXyrmNlMMteEJLkyQjnkX8lh16tTxja1jpbWmrKYWCL9Q1x2fsAMAAAAAAAAChA07AAAAAAAAIEDYsAMAAAAAAAACpFb5KdHl9OnTKrZz504Vs2pWVGZ9uspi1SmgdgEAVF9W3ZKWLVuq2LZt21Ts1KlTYZlTtHPrvoiI9OvXT8VatGjhG2/atEnlhLuGLFCdWNe8Vu3N8ePHl/tYt99+u4q1adNGxR577DHf2KozVFhYGFLMrYvWtGlTlVO7dm0Vc1lzsGqnffDBByr20ksv+cbHjx8v9/mqI3etPfzwwyonJydHxQoKClQsPz//nI8NXCjrmOBef+zfv7+qpoMwCt4OFQAAAAAAAFCNsWEHAAAAAAAABAgbdgAAAAAAAECAsGEHAAAAAAAABEjMNZ0IFc0cAADRyCo0vHz5chU7cuRISD8LkXr16qnYoEGDVKxTp06+sVVkvrS0tNLmBUCzjmNWg7knn3zSN161apXKGTlypIqlpqb6xlZTn+Tk5JBiLqvxz759+1SspKTEN96yZYvKmTdvnoqtWbNGxWgyERr3PvCTTz6J0EwA4N/4hB0AAAAAAAAQIGzYAQAAAAAAAAHChh0AAAAAAAAQIGzYAQAAAAAAAAES54VYgTouLi7cc0EUqarC5aw7nK0qC+az9nC2IB3zatTQ/6/txRdfVLGcnBwV+/Of/+wbnzhx4jxmV700atRIxQoKCqp0DkFad6g+YnXdxcfHq1jz5s1VLCUlxTe+4447VE5F515cXKxiq1evVrGioqJzjkXs49Hp06crNK8g4BoPkRKrxzwEW6jrjk/YAQAAAAAAAAHChh0AAAAAAAAQIGzYAQAAAAAAAAHChh0AAAAAAAAQIDSdQIVQnBORQEFiRErQj3lWIwpLWVlZhR4fkRH0dYfYxLorX61atcL6+KWlpWF9/CDiGg+RwjEPkUDTCQAAAAAAACAKsWEHAAAAAAAABAgbdgAAAAAAAECAhLcAAwAACDtq0wFA1amONeYAAFWPT9gBAAAAAAAAAcKGHQAAAAAAABAgbNgBAAAAAAAAAcKGHQAAAAAAABAgcZ7neZGeBAAAAAAAAIAf8Ak7AAAAAAAAIEDYsAMAAAAAAAAChA07AAAAAAAAIEDYsAMAAAAAAAAChA07AAAAAAAAIEDYsAMAAAAAAAAChA07AAAAAAAAIEDYsAMAAAAAAAAChA07AAAAAAAAIED+H307lY7mRTAyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: CNN Model Design\n",
        "\n",
        "Design a simple CNN to classify the dataset mentioned above. Your network should include at least two convolutional layers, one pooling layer, and a fully connected layer. The model architicture should be in the following order:\n",
        "\n",
        "*   Convolutional Layer 1: Implement a convolutional layer with 32 output channels, a 3x3 kernel size, a stride of 1, and a padding of 1.\n",
        "*   Batch Norm Layer.\n",
        "*   ReLU Activation Layer.\n",
        "*   MaxPooling Layer: Add a max-pooling layer with a 2x2 kernel size and a stride of 2.\n",
        "\n",
        "*   Convolutional Layer 2: Implement a convolutional layer with 64 output channels, a 3x3 kernel size, a stride of 1, and a padding of 1.\n",
        "*   Batch Norm Layer.\n",
        "*   ReLU Activation Layer.\n",
        "*   MaxPooling Layer: like above.\n",
        "\n",
        "*  Fully Connected Layer 1 (Input Size: Dynamically Inferred Based on Previous Layer Output): Include a fully connected (linear) layer with an input size that is dynamically inferred based on the output size of the previous convolutional layer. Output size 128.\n",
        "\n",
        "* RelU Activation Function.\n",
        "\n",
        "*  Fully Connected Layer 2: Add a final fully connected (linear) layer with 26 output features, representing the 26 alphabet letters.\n"
      ],
      "metadata": {
        "id": "KEaSwai7mSb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Define the CNN architecture\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EMNIST_CNN(nn.Module):\n",
        "  def __init__(self, num_classes=26):\n",
        "    super(EMNIST_CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, padding=1, kernel_size=3, stride=1)\n",
        "    self.norm1 = nn.BatchNorm2d(32)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "    self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.norm1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "EnKTVsAOm-A0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Training and Optimization\n",
        "\n",
        "In this section we will setup all training parameters and train the model. your tasks in this section are:\n",
        "1. Create an instance of the model.\n",
        "2. Choose and explain your choice of optimizer and loss function.\n",
        "3. Fill the missing code in the training function.\n",
        "4. Train a model for 5 epochs on the training set (in colab CPU should take ~20 mins).\n",
        "\n",
        "\n",
        "**Note**: EMNIST labels start from 1 to 26, skipping 0. But our models' results are indexed from 0 to 25.\n"
      ],
      "metadata": {
        "id": "LaRj0tiI-875"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "# Check if a GPU is available, otherwise use the CPU\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "N5LtTOYua7F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b71378-5141-4454-e628-df7fb241928e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Create an instance of the model\n",
        "#TODO create an optimizer and loss function\n",
        "model = EMNIST_CNN()\n",
        "model.to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "087Qgsgu_NZ2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO fill the missing code in the training function\n",
        "\n",
        "# training function partially implemented for assignment\n",
        "def train_model(model, loader, optimizer, loss_fun, num_epochs=5):\n",
        "  model.train() #set model to train\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    #create variables to print loss and training accuracy per epoch\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(loader):\n",
        "      images, labels = images.to(device), (labels - 1).to(device)  # EMNIST labels are 1-based\n",
        "\n",
        "\n",
        "\n",
        "      outputs = model(images)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_function(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      #epoch loss and accuracy for printing\n",
        "      epoch_loss += loss.item()\n",
        "      _, predicted = model(images).max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")"
      ],
      "metadata": {
        "id": "efu9zYrwZrT0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO train the model.\n",
        "# training the model for 5 epochs on CPU in colab should take ~20 mins.\n",
        "# print(images.type())\n",
        "# print(next(model.parameters()).type())\n",
        "train_model(model, train_loader, optimizer, loss_function, num_epochs=5)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EfcPQz1HLMPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e663936-7ca6-4cc2-98a2-97e65287eb35"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1950/1950 [00:49<00:00, 39.44it/s]\n",
            "100%|██████████| 1950/1950 [00:46<00:00, 42.25it/s]\n",
            "100%|██████████| 1950/1950 [00:49<00:00, 39.40it/s]\n",
            "100%|██████████| 1950/1950 [00:46<00:00, 42.18it/s]\n",
            "100%|██████████| 1950/1950 [00:48<00:00, 40.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5], Loss: 0.1828, Accuracy: 97.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Evaluate the Model Preformance\n",
        "\n",
        "The following method evaluates a given model on the loaded test set. Use it to evaluate your trained model on the test set loader. And answer the open-ended questions written below."
      ],
      "metadata": {
        "id": "JC27IAIqMyph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in loader:\n",
        "      images, labels = images.to(device), (labels - 1).to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "oXrkaCoUpoib"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO evaluate your model\n",
        "test_accuracy = evaluate_model(model, test_loader)\n",
        "print(f\"Model achieved {test_accuracy:.2f}% accuracy on the test set.\")\n"
      ],
      "metadata": {
        "id": "xzb0nqoWNLjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42623950-ed32-4293-8737-dd58bcfbba8f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.24%\n",
            "Model achieved 93.24% accuracy on the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer the following open ended questions.\n",
        "\n",
        "1. *If the CNN underperforms, what architectural modifications could improve its performance (e.g., depth, kernel size, activation functions)? Justify your choices.*\n",
        "\n",
        "2. *How do the choices of batch size, learning rate, and optimizer affect the training process and final performance of your CNN? Provide examples from your results.*"
      ],
      "metadata": {
        "id": "Fwp-y_lQOqXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5: FeedForward Network\n",
        "\n",
        "In this section you will suggest a feedforward architecture to solve the same problem above, classify the EMNIST images. tasks:\n",
        "\n",
        "\n",
        "1.   Suggest a feedforward model to solve the same problem the CNN above was trained on. Explain and implement your choice in `EMNIST_FeedForward` class\n",
        "2.   Train the model on the same dataset - you can reuse the `train_model` method defined above. Explain your choice of loss function and optimizer. (training a 3 layer model takes approx 5 mins on colab CPU)\n",
        "3.   Evaluate the feedforward model preformance on the test set, again, you can reuse the `evaluate_model` method.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z0bz4Ix6dKSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Feedforward Model"
      ],
      "metadata": {
        "id": "iiXHrCc5Zrni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 6 - Compare & Discuss\n",
        "\n",
        "In this section we will compare the two approaches."
      ],
      "metadata": {
        "id": "lBw3aD8NoCsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Comparing Architectures\n",
        "\n",
        "*After design a feedforward network and a CNN for the same dataset, explain how they differ in terms of:*\n",
        "\n",
        "*   *Number of parameters*\n",
        "*   *Ability to capture spatial relationships*\n",
        "*   *Success/failure in solving the classification task*\n",
        "\n"
      ],
      "metadata": {
        "id": "PUfx9XKgpGfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Visualize Filters & Feature Maps\n",
        "\n",
        "Run the following code sections in order to visualize the filters and feature maps of the first layer in your trained CNN. Answer the questions below afterwards.\n",
        "\n",
        "Notice, you must open the code and specify the name of your CNN model."
      ],
      "metadata": {
        "id": "099CECTMrofB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizing Kernels from the First Convolutional Layer\n",
        "#@markdown Open code to edit model name.\n",
        "\n",
        "cnn_model = cnn_model #ADD YOU MODEL INSTANCE NAME HERE.\n",
        "\n",
        "# Access the first convolutional layer's filters\n",
        "filters = cnn_model.layer1[0].weight.data.cpu()  # Shape: (num_filters, input_channels, F, F)\n",
        "\n",
        "# Number of filters\n",
        "num_filters = filters.shape[0]\n",
        "\n",
        "# Calculate grid dimensions for visualization (rows x cols)\n",
        "grid_cols = 16  # You can adjust this value\n",
        "grid_rows = math.ceil(num_filters / grid_cols)\n",
        "\n",
        "# Create a grid of subplots\n",
        "fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(15, grid_rows * 2))\n",
        "axes = axes.flatten()  # Flatten for easier indexing\n",
        "\n",
        "# Plot each filter\n",
        "for i, ax in enumerate(axes):\n",
        "    if i < num_filters:\n",
        "        # Select the first channel of each filter (assuming grayscale)\n",
        "        filter_ = filters[i, 0, :, :]\n",
        "\n",
        "        # Normalize filter values to [0, 1]\n",
        "        filter_ = (filter_ - filter_.min()) / (filter_.max() - filter_.min())\n",
        "\n",
        "        # Plot the filter\n",
        "        ax.imshow(filter_.numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'Filter {i+1}')\n",
        "    else:\n",
        "        # Turn off unused axes\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D_q_WkeUoa9q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "07cf7ebe-6dc8-4837-a86b-3e8e5f5cc201"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cnn_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-00d9d6195391>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#@markdown Open code to edit model name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m \u001b[0;31m#ADD YOU MODEL INSTANCE NAME HERE.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Access the first convolutional layer's filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizing Feature Maps from the First Convolutional Layer\n",
        "#@markdown Change the image id and run the code cell to view different images.\n",
        "\n",
        "#@markdown Open code to edit model name.\n",
        "image_id = 25  #@param {type: \"slider\", min: 0, max: 63}\n",
        "cnn_model = cnn_model #ADD YOU MODEL INSTANCE NAME HERE.\n",
        "# image_id = 10  #@param {type: \"number\"}\n",
        "\n",
        "# Use a trained model\n",
        "cnn_model.eval()\n",
        "\n",
        "# Load one image from the dataset (assuming the data is normalized)\n",
        "image, _ = next(iter(test_loader))  # Assuming test_loader is defined\n",
        "image = image[image_id].unsqueeze(0).to(device)  # Select an image and add batch dimension\n",
        "\n",
        "# Pass the image through the first convolutional layer\n",
        "with torch.no_grad():\n",
        "    conv1_output = cnn_model.layer1[0](image)  # Assuming 'layer1[0]' is the first conv layer\n",
        "\n",
        "# Get the number of feature maps\n",
        "num_feature_maps = conv1_output.shape[1]\n",
        "\n",
        "# Calculate grid dimensions for visualization\n",
        "grid_cols = 16  # You can adjust this\n",
        "grid_rows = math.ceil(num_feature_maps / grid_cols)\n",
        "\n",
        "# Create a grid of subplots\n",
        "fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(15, grid_rows * 2))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each feature map\n",
        "for i, ax in enumerate(axes):\n",
        "    if i < num_feature_maps:\n",
        "        # Visualize the ith feature map\n",
        "        ax.imshow(conv1_output[0, i].cpu().numpy(), cmap='viridis')\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'Feature {i+1}')\n",
        "    else:\n",
        "        # Turn off unused axes\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oWLM4xWOwAE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Are Feature Maps?**\n",
        "\n",
        "  Each feature map corresponds to the activation of one filter in the convolutional layer.\n",
        "\n",
        "  Filters detect specific features from the input image.\n",
        "\n",
        "  The values in a feature map indicate how strongly a specific feature was detected at each spatial location in the input.\n",
        "\n",
        "\n",
        "**Understanding the Color Map**\n",
        "\n",
        "  The Viridis color map (default in Matplotlib) uses a gradient of colors:\n",
        "\n",
        "  *  Purple (low values): Indicates weak or no activation at a particular spatial location. The filter didn't find much of its target feature in this area.\n",
        "  *  Green (high values): Indicates strong activation. The filter detected its target feature strongly in this region of the input.\n",
        "\n",
        "Given the visualizations and explanation above, answer the following questions:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cH3-oPXXy-A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1. Are there any filters that have almost no activation across all features maps? Assume there are such filters, what might this indicate about the training process or the architecture?*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCEHSxcr03o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2. Based on the activations in the feature maps, do you think the model is focusing on important parts of the input image? How could you modify the model if you suspect it's not focusing on the right features?*"
      ],
      "metadata": {
        "id": "fo9pbzsQ1Vt2"
      }
    }
  ]
}